{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the specified path\n",
    "dataset_path = '/content/drive/MyDrive/AI/RAG/spam_detection/spam_ham_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows of the dataset to check\n",
    "df.head()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for all the emails in the dataset\n",
    "email_embeddings = [model.encode(email) for email in df['text']]\n",
    "\n",
    "# Convert the list of embeddings into a numpy array\n",
    "email_embeddings = np.array(email_embeddings)\n",
    "\n",
    "!pip install transformers sentence-transformers langchain pinecone-client[grpc] faiss-cpu\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Get the embedding dimension\n",
    "dimension = email_embeddings.shape[1]\n",
    "\n",
    "# Create the FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add the email embeddings to the index\n",
    "index.add(email_embeddings)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare the labels (0 for ham, 1 for spam)\n",
    "labels = df['label_num'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(email_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "base_dir = \"/content/drive/MyDrive/AI/RAG/spam_detection\"\n",
    "dataset_path = f\"{base_dir}/spam_ham_dataset.csv\"\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Clean email text\n",
    "data['cleaned_text'] = data['text'].apply(\n",
    "    lambda x: re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    ")\n",
    "\n",
    "# Load pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and efficient\n",
    "\n",
    "# Generate embeddings\n",
    "data['embeddings'] = data['cleaned_text'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Convert embeddings into a numpy array\n",
    "embeddings = np.array(data['embeddings'].tolist()).astype('float32')\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
    "\n",
    "# Function to search for similar emails\n",
    "def search_similar_emails(query_embedding, k=5):\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, k)  # k nearest neighbors\n",
    "    return indices, distances\n",
    "\n",
    "# Sample spam classification function using Gemini API\n",
    "def gemini_llm(prompt):\n",
    "    api_key = 'AIzaSyA0t6XAKE7eNH8DyCGOvD2ntQKPOMJq830'  # Replace with your actual API key\n",
    "    gemini_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\"\n",
    "\n",
    "    # Send request to Gemini API\n",
    "    response = requests.post(\n",
    "        gemini_url,\n",
    "        headers={'Content-Type': 'application/json'},\n",
    "        params={'key': api_key},\n",
    "        json={\n",
    "            \"contents\": [{\n",
    "                \"parts\": [{\"text\": prompt}]\n",
    "            }]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Extract the response text\n",
    "    response_data = response.json()\n",
    "    if 'candidates' in response_data:\n",
    "        return response_data['candidates'][0]['content']['parts'][0]['text']\n",
    "    else:\n",
    "        return \"Error in response\"\n",
    "\n",
    "# Function to classify an email and build context using the retrieved similar emails\n",
    "def classify_email(email_text):\n",
    "    query_embedding = model.encode(email_text)\n",
    "    indices, _ = search_similar_emails(query_embedding)\n",
    "\n",
    "    # Build context from retrieved similar emails\n",
    "    context = \" \".join(data.iloc[idx]['text'] for idx in indices[0])\n",
    "\n",
    "    # Create the classification prompt\n",
    "    prompt = f\"Based on the following context and email:\\nContext: {context}\\nEmail: {email_text}\\nClassify as spam or not spam.\"\n",
    "\n",
    "    # Use the Gemini API to classify the email\n",
    "    return gemini_llm(prompt)\n",
    "\n",
    "# Test with a sample email\n",
    "sample_email = \"Click here to claim your free gift card now!\"\n",
    "classified_result = classify_email(sample_email)\n",
    "print(\"Spam Classification Result:\", classified_result)\n",
    "\n",
    "# Save the model to Google Drive\n",
    "import pickle\n",
    "\n",
    "# Define the path to save the model in Google Drive\n",
    "model_save_path = '/content/drive/MyDrive/AI/RAG/spam_detection/RAGgpu_model.pkl'  # Adjust to your desired path\n",
    "\n",
    "# Save the RAG model using pickle\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"RAG model saved to {model_save_path}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "\n",
    "# Load the saved model\n",
    "with open('/content/drive/MyDrive/AI/RAG/spam_detection/RAGgpu_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Function to classify an email (same as before)\n",
    "def classify_email(email_text, model, k=5):\n",
    "    query_embedding = model.encode(email_text)\n",
    "    indices, _ = search_similar_emails(query_embedding, k)\n",
    "\n",
    "    # Build context from retrieved similar emails\n",
    "    context = \" \".join(data.iloc[idx]['text'] for idx in indices[0])\n",
    "\n",
    "    # Create the classification prompt\n",
    "    prompt = f\"Based on the following context and email:\\nContext: {context}\\nEmail: {email_text}\\nClassify as spam or not spam.\"\n",
    "\n",
    "    # Use the Gemini API to classify the email\n",
    "    return gemini_llm(prompt)\n",
    "\n",
    "# Generate predictions for the entire dataset\n",
    "predictions = []\n",
    "for email in data['text']:\n",
    "    result = classify_email(email, model=loaded_model)\n",
    "    # Assuming the result returned by Gemini API is either \"spam\" or \"not spam\"\n",
    "    predictions.append('spam' if 'spam' in result.lower() else 'ham')\n",
    "\n",
    "# Compare predictions with actual labels\n",
    "y_true = data['label'].apply(lambda x: 'spam' if x == 1 else 'ham').tolist()  # Assuming label is 1 for spam, 0 for ham\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, predictions)\n",
    "precision = precision_score(y_true, predictions, pos_label='spam')\n",
    "recall = recall_score(y_true, predictions, pos_label='spam')\n",
    "f1 = f1_score(y_true, predictions, pos_label='spam')\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Debugging predictions\n",
    "for email, true_label, pred in zip(data['text'][:10], y_true[:10], predictions[:10]):\n",
    "    print(f\"Email: {email}\\nTrue Label: {true_label}\\nPredicted: {pred}\\n\")\n",
    "\n",
    "# Check label distribution\n",
    "print(\"Label Distribution:\")\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "# Inspect a sample API response\n",
    "sample_response = classify_email(data['text'][0], model=loaded_model)\n",
    "print(\"Sample API Response:\", sample_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
